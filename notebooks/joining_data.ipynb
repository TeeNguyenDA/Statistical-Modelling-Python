{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data from Part 1 with the data from Part 2 to create a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv files from part 2 with Foursquare and Yelp EDA\n",
    "cb = pd.read_csv('../data/cb_montreal_stations.csv')\n",
    "fsq = pd.read_csv('../data/fsq_bar_features.csv')\n",
    "yelp = pd.read_csv('../data/yelp_bar_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq.sort_values(by='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp.sort_values(by='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq['name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations here:\n",
    "\n",
    "* There are more unique bar names in the 'fsq' dataframe.\n",
    "* The category of bars from 'fsq' is better categorized, because Foursquare already has a taxonomy of well-define category. We used search string \"bars\" for categories in the Yelp API call, thus it's not as well-organized. We also added \n",
    "* The 'distance' column in Yelp has values greater than 1000m, although we set the radius=1000 only.\n",
    "\n",
    "For the reasons above, we'll use bar information from Foursquare as the base of our clean data; then add additional information such as 'review_count', 'rating', 'price' from Yelp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to add 'review_count', 'rating', 'price' columns from the 'yelp' to 'fsq' dataframe\n",
    "fsq = fsq.merge(yelp[['name', 'review_count', 'rating', 'price']].drop_duplicates(subset=['name']), on='name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again the new 'fsq' dataframe after adding 'review_count', 'rating', 'price' from Yelp\n",
    "fsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to merge the new 'fsq' with the Citybikes data in the 'cb' dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'cb' and 'fsq' based on 'cb_latitude' and 'cb_longitude' in 'cb' and 'latitude' and 'longitude' in 'fsq'\n",
    "merged_all_df = pd.merge(cb, fsq, left_on=['cb_latitude', 'cb_longitude'], right_on=['latitude', 'longitude'], how='inner')\n",
    "\n",
    "# Drop the repeated 'latitude' and 'longitude' columns from the merged DataFrame\n",
    "merged_all_df = merged_all_df.drop(['latitude', 'longitude'], axis=1)\n",
    "\n",
    "merged_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our master dataframe 'merged_all_df' results to csv\n",
    "merged_all_df.to_csv('../data/merged_all_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a visualization that you used as part of your EDA process. Explain the initial pattern or relationship you discoved through this visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'unique_name_count' to store the count of unique bars per citybike station in Montreal\n",
    "merged_all_df['cb_name_count'] = merged_all_df.groupby('cb_station_name')['name'].transform('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where the missing values are located\n",
    "merged_all_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Treat missing values in 'postcode':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp[['name','postcode']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of 'name' to 'postcode' from the Yelp dataframe above. It has a bigger dataset than Foursquare\n",
    "unique_yelp_postcode_mapping = yelp[['name', 'postcode']].drop_duplicates().set_index('name')['postcode'].to_dict()\n",
    "\n",
    "# Use the mapping to fill missing 'postcode' values in merged_all_df\n",
    "merged_all_df['postcode'].fillna(merged_all_df['name'].map(unique_yelp_postcode_mapping), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df['postcode'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the rest of 176 missing values in 'postcode' with 'Postcode Unavailable'\n",
    "merged_all_df['postcode'].fillna('Unavailable', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Treat missing values in 'review_count', 'rating':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 graphs to visualize the distribution of  'review_count' and 'rating':\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n",
    "\n",
    "# Plot 0\n",
    "axes[0].hist(merged_all_df['review_count'])\n",
    "axes[0].set_title('Histogram of review_count')\n",
    "axes[0].set_xlabel('review_count')\n",
    "axes[0].set_ylabel('counts')\n",
    "\n",
    "# Plot 1\n",
    "axes[1].hist(merged_all_df['rating'], color='green')\n",
    "axes[1].set_title('Histogram of rating')\n",
    "axes[1].set_xlabel('rating')\n",
    "axes[1].set_ylabel('counts')\n",
    "axes[1].set_xlim(left=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the distributions of these two columns are skewed, we will fill in the missing values with the median.\n",
    "merged_all_df['review_count'].fillna(merged_all_df['review_count'].median(), inplace=True)\n",
    "merged_all_df['rating'].fillna(merged_all_df['rating'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Treat missing values in 'price':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df['price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of each price category\n",
    "(merged_all_df['price'].value_counts() / merged_all_df['price'].notnull().sum()) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '$$' value is most populated with > 80%, we'll use it to fill in missing values in this column\n",
    "merged_all_df['price'].fillna('$$', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this paper at: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0252157#:~:text=Yelp%20use%20dollar%20signs%20(%24),higher%20level%20means%20more%20expensive\n",
    "\n",
    "\"Yelp uses dollar signs ($) to categorize approximate cost per person for a meal in a restaurant. In particular, “$” means under $10; “$$” means “$11-$30”; “$$$” means “$31-$60”; and “$$$$” means “above $61”. \"\n",
    "\n",
    "We can reassign the funky dollar sign values in 'price' column into a categorical system that takes 1 to 4 as price levels. Higher level means more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_price_level(price):\n",
    "    if price == '$':\n",
    "        return 1\n",
    "    elif price == '$$':\n",
    "        return 2\n",
    "    elif price == '$$$':\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "merged_all_df['price_level'] = merged_all_df['price'].apply(assign_price_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'price' as we already indicated the same thing in 'price_level'\n",
    "merged_all_df = merged_all_df.drop('price', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check for missing values\n",
    "merged_all_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates (if any) after treating n/a, but there aren't any because cb_station_id is unique\n",
    "merged_all_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exploring numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = merged_all_df.select_dtypes(include=['number']).columns\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_merge_df = merged_all_df[numeric_cols]\n",
    "numeric_merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histograms(dataframe, features, rows, cols):\n",
    "    fig=plt.figure(figsize=(20,20))\n",
    "    for i, feature in enumerate(features):\n",
    "        ax=fig.add_subplot(rows, cols, i+1)\n",
    "        dataframe[feature].hist(bins=20, ax=ax, facecolor='Midnightblue')\n",
    "        ax.set_title(feature + \" Distribution\",color='DarkRed')\n",
    "        ax.set_yscale('log')\n",
    "    fig.tight_layout()  \n",
    "    plt.show()\n",
    "    \n",
    "draw_histograms(numeric_merge_df,numeric_merge_df.columns,8,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still bar locations that are further out than 1Km from Montreal bike stations. The distribution of 'cb_bike_num' is skewed with outliers.\n",
    "\n",
    "As per characteristic of bars (of numeric type except for 'price_level'): 'distance' looks somewhat like a uniform distribution. The distribution of 'review_count' looks skewed, 'rating' has a pretty good shape of normal distribution without outliers. \n",
    "\n",
    "We are dealing with outliers, and we can also test the normal distribution of some of those numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce the condition 'distance'values must be <= 1000 from the bike stations\n",
    "merged_all_df = merged_all_df[merged_all_df['distance'] <= 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Treating outliers: We won't take into accounts outliers in 'cb_latitude' and 'cb_longitude' since we want to include all bike stations in Montreal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of rows with outliers where 'cb_bike_num' >50\n",
    "len(merged_all_df[merged_all_df['cb_bike_num'] <= 50])/len(merged_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of rows with outliers where 'review_count' >300\n",
    "len(merged_all_df[merged_all_df['review_count'] <= 300])/len(merged_all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll lose around 1% of our data (~60 rows) for each outlier removal which is quite ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extreme outliers where 'cb_bike_num' >50\n",
    "merged_all_df = merged_all_df[merged_all_df['cb_bike_num'] <= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extreme outliers where 'review_count' >300\n",
    "merged_all_df = merged_all_df[merged_all_df['review_count'] <= 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform Shapiro-Wilk's normality test on 'cb_bike_num', 'cb_name_count', 'distance', 'review_count', 'rating':\n",
    "- H<sub>0</sub>: data is normally distributed\n",
    "- H<sub>a</sub>: data is not normally distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = stats.shapiro(merged_all_df['cb_bike_num'])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = stats.shapiro(merged_all_df['cb_name_count'])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = stats.shapiro(merged_all_df['distance'])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = stats.shapiro(merged_all_df['review_count'])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = stats.shapiro(merged_all_df['rating'])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p<<0.05, we reject the null hypothesis that the data in these columns are normally distributed. We'll have to perform data normalization at a latter step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exploring categorical columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which station has the most bars close by?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df['cb_station_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df['cb_station_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_barnum_sorted = merged_all_df.groupby(['cb_station_id', 'cb_station_name'])['cb_name_count'].mean().sort_values(ascending=False).reset_index()\n",
    "stations_barnum_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The majority of numbar of bars associated per station is 10 bars on average\n",
    "stations_barnum_sorted['cb_name_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(4,2))\n",
    "sns.boxplot(x=stations_barnum_sorted['cb_name_count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top stations with the most number of bars will be mostly those with 10 bars nearby\n",
    "top_barnum_stations = stations_barnum_sorted[stations_barnum_sorted['cb_name_count']==10]\n",
    "top_barnum_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any relationship between the bike station name and the bar postcode and categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_postcode = merged_all_df[['cb_station_name','postcode']].drop_duplicates()\n",
    "stations_postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_postcode['postcode_prefix'] = stations_postcode['postcode'].str[:3]\n",
    "stations_postcode = stations_postcode[stations_postcode['postcode_prefix'] != 'Una'].drop('postcode', axis=1).drop_duplicates()\n",
    "stations_postcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's unclear if there's any relationship between the bike station name and the bar postcode and categories at this point, because of the large number of unique combinations. Visualization won't be the best option to carry out EDA for our categorical columns overall. To keep things simpler, it's better to just use numerical columns for prediction in our model in part 4, and not going forward with encoding categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in the previous section of data visualization, all of our numeric columns have p value = 0 or very small in the normality test. However, we won't apply data transformation to convert those numeric columns into normal distribution within our scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(merged_all_df.corr(), annot = True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see that it seems like there may be a very small linear correlation between our target variable ('cb_bike_num') and these bar characteristics:\n",
    "\n",
    "* Minor positive correlation with 'review_count' (0.18), 'price level' (0.065)\n",
    "* Minor negative correlation with 'distance'(-0.11), 'rating' (-0.019)\n",
    "\n",
    "A correlation near zero indicates the variables are not linearly related. Thus, we can exclude the variables with near zero correlation. Thus, the dependent variables kept in our model to predict our our target variable ('cb_bike_num') are 'review_count', 'distance'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all your results in an SQLite3 database (remember, SQLite stores its databases as files in your local machine - make sure to create your database in your project's data/ directory!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our scope for this SQLite3 database in Python for part 3:\n",
    "\n",
    "* Create connection and access data from three API results (dataframes) that we have from part 2 into 3 tables\n",
    "* Use SQL queries to access data from those 3 tables\n",
    "* Join data from three multiple tables into a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "import sqlite3\n",
    "from sqlite3 import Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create connection and access data from three API results (dataframes) that we have from part 2 into 3 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_connection(path):\n",
    "    \"\"\" Create a database connection to a SQLite database \"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = sqlite3.connect(path)\n",
    "        print('Connection to SQLite DB successful')\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty database named 'merged_all_sql.db' in '../data/' if not exists\n",
    "connection = create_connection('../data/merged_all_sql.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new set of dataframes again for citybikes foursquare and yelp data\n",
    "cb_sql_df = pd.read_csv('../data/cb_montreal_stations.csv')\n",
    "fsq_sql_df = pd.read_csv('../data/fsq_bar_features.csv')\n",
    "yelp_sql_df = pd.read_csv('../data/yelp_bar_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to SQLite tables: 'citybikes', 'foursquare', 'yelp' without index\n",
    "cb_sql_df.to_sql('citybikes', connection, if_exists='replace', index='False')\n",
    "fsq_sql_df.to_sql('foursquare', connection, if_exists='replace', index='False')\n",
    "yelp_sql_df.to_sql('yelp', connection, if_exists='replace', index='False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use SQL queries to access data from those 3 tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data before and after the join to validate your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function execute_read_query()\n",
    "def execute_read_query(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    result = None\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        return result\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query data from the 'citybikes' table\n",
    "select_citybikes = \"SELECT * from citybikes\"\n",
    "cb_rows = execute_read_query(connection, select_citybikes)\n",
    "\n",
    "for cb_row in cb_rows:\n",
    "    print(cb_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query data from the 'foursquare' table\n",
    "select_foursquare = \"SELECT * from foursquare\"\n",
    "fsq_rows = execute_read_query(connection, select_foursquare)\n",
    "\n",
    "for fsq_row in fsq_rows:\n",
    "    print(fsq_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query data from the 'yelp' table\n",
    "select_yelp = \"SELECT * from yelp\"\n",
    "yelp_rows = execute_read_query(connection, select_yelp)\n",
    "\n",
    "for yelp_row in yelp_rows:\n",
    "    print(yelp_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Join data from three multiple tables into a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same concept as in the pandas joining tables\n",
    "# Add 'review_count', 'rating', 'price' columns from the 'yelp' to 'fsq' dataframe\n",
    "\n",
    "left_join_foursquare_yelp = '''\n",
    "CREATE VIEW left_join_foursquare_yelp\n",
    "AS \n",
    "SELECT foursquare.*\n",
    "FROM foursquare\n",
    "LEFT JOIN (\n",
    "    SELECT DISTINCT name, review_count, rating, price\n",
    "    FROM yelp\n",
    ") As tbl ON foursquare.name = tbl.name;\n",
    "'''\n",
    "\n",
    "fsq_yelp_rows = execute_read_query(connection, left_join_foursquare_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view that combines the 'left_join_foursquare_yelp' view and 'citybikes' table\n",
    "\n",
    "foursquare_yelp_citybikes = '''\n",
    "CREATE VIEW merge_foursquare_yelp_citybikes\n",
    "AS \n",
    "SELECT *\n",
    "FROM left_join_foursquare_yelp\n",
    "INNER JOIN citybikes AS cb\n",
    "ON cb.cb_latitude = left_join_foursquare_yelp.latitude\n",
    "AND cb.cb_longitude = left_join_foursquare_yelp.longitude;\n",
    "'''\n",
    "\n",
    "foursquare_yelp_citybikes = execute_read_query(connection, foursquare_yelp_citybikes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
